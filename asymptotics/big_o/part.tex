\subsection{Сравнение различных программ и $O$ большое}
Ключевой характеристикой программы является время её работы. Оно, по сути, прямо пропорционально количеству операций, которые необходимо произвести компьютеру перед тем, как программа прекратит свою работу. Чаще всего это количество некоторым образом зависит от количества данных, подаваемых на вход программе. Для последующих рассуждений обозначим эту зависимость как $f(n)$, где $n$ --- количество входных данных.

Итак, для оценки времени работы программы нужно знать \underline{\emph{точное}} количество операций, которые она затребует после запуска. В особо острых ситуациях, когда очень важно быстродействие, так и есть, но, бывает, можно обойтись и следующей более грубой оценкой: будем считать, что программа имеет линейное время работы, если при достаточно больших значениях $n$ значение $f(n)$ не превосходит значение другой функции $g(n) = C \cdot n$, где $C$ --- это ненулевая конечная константа. Или, более строго, для некоторых ненулевых конечных констант $N$ и $C$, для любого $n$, большего $N$, выполняется $f(n) \le C \cdot n$.
То есть:
$$\frac{f(n)}{n} \le C.$$
Если выполняется это условие, то считается, что время работы алгоритма --- $O(n)$.

Помимо линейной функции можно использовать и любые другие. Например, если
$$\frac{f(n)}{n^3} \le C,$$
при тех же ограничениях на $n$, $C$, и $N$, что и до этого, то время работы алгоритма --- $O(n^3)$.

Таким же образом можно оценить и количество памяти, задействованной программой.

\subsection*{Замечание 1}
Часто встречаются алгоритмы, время работы которых --- $O(\log_2 n)$. В таких случаях основание логарифма обычно не пишут, так как в этой нотации оно может быть заменено другим числом, не равным единице, (обозначим это число как $a$). В самом деле, по свойству логарифма имеем:
$$\frac{\log_2 n}{\log_2 a} = \log_a n$$
$$\frac{f(n)}{\log_2 n} \le C \Rightarrow \frac{f(n)}{\log_2 n} = \frac{f(n)}{\log_a n \cdot \log_2 a} \le C \Rightarrow \frac{f(n)}{\log_a n} \le C \cdot \log_2 a.$$
Так как $C \cdot \log_2 a$ --- тоже ненулевая конечная константа, получаем, что время работы того же самого алгоритма --- не только $O(\log_2 n)$, но ещё и $O(\log_a n)$, а потому константа в основании логарифма не существенна и обычно отбрасывается.

\subsection*{Замечание 2}
Если значение $f(n)$ не превосходит некоторую ненулевую конечную константу $C$, то считается, что время работы алгоритма --- $O(1)$, так как
$$\frac{f(n)}{1} \le C.$$
\newpage
\subsection*{Замечание 3}
Если известны ограничения на $n$ (например, что $n$ --- целое число, которое обязательно находится в некотором конечном промежутке $[a; b]$) и алгоритм всегда завершает свою работу за конечное число шагов (то есть $\forall n \in [a; b] \hspace{3mm} f(n) < \infty$), то, имея определённую смелость, можно заявить, что время работы программы --- $O(1)$, так как
$$\frac{f(n)}{1} \le C = \max_{n \in [a; b]}{f(n)}.$$
Таким образом, любой конечный алгоритм работает за константное время, если взять достаточно большую константу. По понятным причинам, такая оценка применяется нечасто.
